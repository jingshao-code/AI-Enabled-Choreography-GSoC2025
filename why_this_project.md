# Why This Project

When I first came across the "AI-Enabled Choreography: Dance Beyond Music" project, I felt an immediate connection. It brings together so many things I care about—AI, movement, expression, and creative tools that help people connect more deeply with art. With my background in 3D modeling, embodied AI research, and multimodal systems, this project feels like a natural fit for me—both technically and personally.

Dance has been a part of my life since I was eight years old. I started with Latin dance and later moved into jazz, and the experience of expressing emotion through movement has stayed with me ever since. I also worked part-time as a painting instructor, and I’ve always loved exploring artistic expression—whether through motion, visuals, or sound.

My recent research has focused on large-scale audio-language models, particularly around safety and robustness. I’ve worked on adversarial attacks, speech modulation, and the limits of audio understanding in AI systems. I've also used generative tools to create visual art—turning prompts into illustrations and using them in UI design. These experiences helped me see how AI can support creativity, not replace it.

What I love about this project is that it sees dance not just as data, but as embodied experience. AI, when used with care, can connect modalities like movement, sound, and speech in ways that feel intuitive and meaningful. Of course, there are risks—AI can flatten artistic style or lead to over-standardization. But that’s why I appreciate the artist-led vision here: it’s about expanding what’s possible, not simplifying what already exists.

I’m comfortable working independently, but I also love collaborating in small teams. I prefer async tools like Git and Slack, but I’m currently based in the Bay Area—so I’m also happy to jump into real-time meetings or even meet in person when helpful. Interestingly, one of the project mentors, Ilya Vidrin, is from my current university (Northeastern), and the other mentor, Mariel Pettee, is at Lawrence Berkeley National Lab. That local connection makes me even more excited about the potential to stay involved.

One idea I’d love to explore is using **spoken voice**—not just text—as a way to guide movement generation. Can we map tone, pace, and emotion in speech to different dance styles? I think this could unlock new ways for people to create and communicate through motion.

Thanks so much for reading. I’d be honored to contribute to this beautiful mix of machine learning and movement.