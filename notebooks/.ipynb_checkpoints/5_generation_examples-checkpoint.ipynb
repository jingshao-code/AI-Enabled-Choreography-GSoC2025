{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5_generation_examples.ipynb\n",
    "\n",
    "# Cell 1: Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "from src.data.loader import MotionDataLoader\n",
    "from src.visualization.animator import DanceAnimator\n",
    "from src.model.encoders import DanceEncoder, SimpleTextEncoder\n",
    "from src.generation.dance_from_text import retrieve_dance_by_text\n",
    "from src.generation.text_from_dance import retrieve_text_by_dance, generate_composite_description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Introduction\n",
    "\"\"\"\n",
    "# Dance and Text Generation Examples\n",
    "\n",
    "This notebook demonstrates how to use our trained contrastive learning model for bidirectional generation:\n",
    "\n",
    "1. **Text-to-Dance**: Given a text description, find the most similar dance sequence\n",
    "2. **Dance-to-Text**: Given a dance sequence, generate an appropriate text description\n",
    "\n",
    "These examples illustrate the practical applications of our multimodal embedding space, showing how the model can bridge the gap between movement and language.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 304 test embeddings\n",
      "Loaded dataset with 2022 sequences and 2022 labels\n",
      "Detected vocabulary size from saved model: 32\n",
      "Successfully loaded trained model from ../results/models/contrastive_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load trained model and data\n",
    "# Load embeddings saved during training\n",
    "try:\n",
    "    test_dance_embeddings = np.load('../results/embeddings/test_dance_embeddings.npy')\n",
    "    test_text_embeddings = np.load('../results/embeddings/test_text_embeddings.npy')\n",
    "    print(f\"Loaded {len(test_dance_embeddings)} test embeddings\")\n",
    "except:\n",
    "    print(\"Test embeddings not found. Please run the model training notebook first.\")\n",
    "    test_dance_embeddings = None\n",
    "    test_text_embeddings = None\n",
    "\n",
    "# Load sequences and labels\n",
    "try:\n",
    "    # Try to load the labeled dataset\n",
    "    dataset = np.load('../data/processed/labeled_dataset.npy', allow_pickle=True).item()\n",
    "    sequences = dataset['sequences']\n",
    "    all_labels = dataset['labels']\n",
    "    print(f\"Loaded dataset with {len(sequences)} sequences and {len(all_labels)} labels\")\n",
    "except:\n",
    "    # Alternative: try loading the individual components\n",
    "    try:\n",
    "        sequences = np.load('../data/processed/dance_sequences.npy')\n",
    "        \n",
    "        # Try to load labels\n",
    "        try:\n",
    "            label_data = np.load('../data/processed/sequence_labels.npy', allow_pickle=True).item()\n",
    "            all_labels = label_data['labels']\n",
    "        except:\n",
    "            print(\"Labels not found. Creating placeholder labels for demonstration.\")\n",
    "            # Create placeholder labels if needed\n",
    "            all_labels = [f\"Dance movement {i}\" for i in range(len(sequences))]\n",
    "        \n",
    "        print(f\"Loaded {len(sequences)} sequences and {len(all_labels)} labels\")\n",
    "    except:\n",
    "        print(\"No dataset files found. Please run the previous notebooks first.\")\n",
    "        sequences = None\n",
    "        all_labels = None\n",
    "\n",
    "# Load the model (define architecture first)\n",
    "if 'sequences' in locals() and sequences is not None:\n",
    "    # Model parameters\n",
    "    n_joints, seq_length, n_dims = sequences[0].shape\n",
    "    embedding_dim = 128  # Same as during training\n",
    "    \n",
    "    # Try to determine the correct vocab size from the saved model\n",
    "    model_path = '../results/models/contrastive_model.pt'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # First check if the model file exists\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            # Load the checkpoint to inspect dimensions\n",
    "            checkpoint = torch.load(model_path, map_location=device)\n",
    "            # Get text encoder input size from saved weights\n",
    "            if 'text_encoder' in checkpoint:\n",
    "                first_layer_weight = checkpoint['text_encoder']['layers.0.weight']\n",
    "                vocab_size = first_layer_weight.shape[1]\n",
    "                print(f\"Detected vocabulary size from saved model: {vocab_size}\")\n",
    "            else:\n",
    "                # Fallback value\n",
    "                vocab_size = 32\n",
    "                print(f\"Using default vocabulary size: {vocab_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inspecting model file: {e}\")\n",
    "            vocab_size = 32\n",
    "            print(f\"Using default vocabulary size: {vocab_size}\")\n",
    "    else:\n",
    "        # If no model file, use an estimate\n",
    "        vocab_size = 32\n",
    "        print(f\"Model file not found. Using default vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create encoders (we'll load the trained weights)\n",
    "    dance_encoder = DanceEncoder(\n",
    "        n_joints=n_joints,\n",
    "        seq_length=seq_length,\n",
    "        n_dims=n_dims,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    \n",
    "    text_encoder = SimpleTextEncoder(\n",
    "        input_dim=vocab_size,  # Now using the correct vocabulary size\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    \n",
    "    # Try to load model weights\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=device)\n",
    "            dance_encoder.load_state_dict(checkpoint['dance_encoder'])\n",
    "            text_encoder.load_state_dict(checkpoint['text_encoder'])\n",
    "            print(f\"Successfully loaded trained model from {model_path}\")\n",
    "            \n",
    "            # Move to appropriate device\n",
    "            dance_encoder.to(device)\n",
    "            text_encoder.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Using untrained model for demonstration\")\n",
    "    else:\n",
    "        print(f\"Model not found at {model_path}, using untrained model for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 02:33:20,412 - src.visualization.animator - INFO - DanceAnimator initialized with custom settings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: \"smooth flowing movement with arms\"\n",
      "Found matching sequence 216 with similarity score 0.6455\n",
      "Original label: \"energetic leap with whole body moving high\"\n",
      "Animation would be displayed here in notebook\n",
      "\n",
      "Query: \"energetic jump with the whole body\"\n",
      "Found matching sequence 303 with similarity score 0.5965\n",
      "Original label: \"fluid stretch with arms moving diagonal\"\n",
      "Animation would be displayed here in notebook\n",
      "\n",
      "Query: \"sharp kick with legs forward\"\n",
      "Found matching sequence 303 with similarity score 0.6269\n",
      "Original label: \"fluid stretch with arms moving diagonal\"\n",
      "Animation would be displayed here in notebook\n",
      "\n",
      "Query: \"gentle turning motion\"\n",
      "Found matching sequence 218 with similarity score 0.6220\n",
      "Original label: \"energetic leap with whole body moving high\"\n",
      "Animation would be displayed here in notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jing/choreo-ai-project/notebooks/../src/generation/dance_from_text.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens, device=device)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Text-to-Dance Generation\n",
    "if 'dance_encoder' in locals() and 'text_encoder' in locals() and 'sequences' in locals() and 'test_dance_embeddings' in locals():\n",
    "    \"\"\"\n",
    "    ## Text to Dance Generation\n",
    "    \n",
    "    Given a text description, we can find dance sequences that match by:\n",
    "    1. Encoding the text with our text encoder\n",
    "    2. Computing similarity with all pre-encoded dance sequences\n",
    "    3. Retrieving the closest dance sequence(s)\n",
    "    \n",
    "    Let's try some examples using our test set embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a simple text tokenizer function\n",
    "    def simple_tokenizer(text_list):\n",
    "        \"\"\"\n",
    "        Convert text to bag-of-words representation\n",
    "        \n",
    "        Args:\n",
    "            text_list: List of text descriptions\n",
    "            \n",
    "        Returns:\n",
    "            Bag-of-words tensor and None for lengths\n",
    "        \"\"\"\n",
    "        bow_vectors = torch.zeros(len(text_list), vocab_size, device=device)\n",
    "        \n",
    "        # For each text, create a BoW representation\n",
    "        for i, text in enumerate(text_list):\n",
    "            words = text.lower().strip().split()\n",
    "            for word in words:\n",
    "                # We don't have the actual vocabulary, so hash the words\n",
    "                # This is just for demonstration\n",
    "                word_idx = hash(word) % (vocab_size - 2) + 2  # +2 to avoid special tokens\n",
    "                bow_vectors[i, word_idx] = 1\n",
    "        \n",
    "        return bow_vectors, None\n",
    "    \n",
    "    # Create animator for visualization\n",
    "    animator = DanceAnimator(\n",
    "        figsize=(10, 8),\n",
    "        joint_color='blue',\n",
    "        line_color='red',\n",
    "        draw_floor=True,\n",
    "        show_trajectory=True\n",
    "    )\n",
    "    \n",
    "    # Test some example queries\n",
    "    test_queries = [\n",
    "        \"smooth flowing movement with arms\",\n",
    "        \"energetic jump with the whole body\",\n",
    "        \"sharp kick with legs forward\",\n",
    "        \"gentle turning motion\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: \\\"{query}\\\"\")\n",
    "        \n",
    "        # Retrieve matching dance\n",
    "        results = retrieve_dance_by_text(\n",
    "            query,\n",
    "            text_encoder,\n",
    "            simple_tokenizer,\n",
    "            sequences,\n",
    "            test_dance_embeddings,\n",
    "            top_k=1,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Display result\n",
    "        if results:\n",
    "            idx, score, seq = results[0]\n",
    "            print(f\"Found matching sequence {idx} with similarity score {score:.4f}\")\n",
    "            print(f\"Original label: \\\"{all_labels[idx]}\\\"\")\n",
    "            \n",
    "            # Create animation (but don't display in this code snippet)\n",
    "            print(\"Animation would be displayed here in notebook\")\n",
    "            # ani = animator.animate_sequence(seq)\n",
    "            # display(HTML(ani.to_jshtml()))\n",
    "        else:\n",
    "            print(\"No matching sequence found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dance Sequence 231:\n",
      "Top matches:\n",
      "  1. \"energetic leap with whole body moving high\" (score: 0.6472)\n",
      "  2. \"energetic leap with whole body moving high\" (score: 0.6472)\n",
      "  3. \"energetic leap with whole body moving high\" (score: 0.6472)\n",
      "\n",
      "Composite description: \"leap\"\n",
      "Animation would be displayed here in notebook\n",
      "\n",
      "Dance Sequence 6:\n",
      "Top matches:\n",
      "  1. \"fluid stretch with arms moving diagonal\" (score: 0.5948)\n",
      "  2. \"quick turn with torso moving circular\" (score: 0.5948)\n",
      "  3. \"fluid stretch with arms moving diagonal\" (score: 0.5948)\n",
      "\n",
      "Composite description: \"fluid stretch and turn\"\n",
      "Animation would be displayed here in notebook\n",
      "\n",
      "Dance Sequence 238:\n",
      "Top matches:\n",
      "  1. \"energetic leap with whole body moving high\" (score: 0.6282)\n",
      "  2. \"energetic leap with whole body moving high\" (score: 0.6282)\n",
      "  3. \"energetic leap with whole body moving high\" (score: 0.6282)\n",
      "\n",
      "Composite description: \"leap\"\n",
      "Animation would be displayed here in notebook\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Dance-to-Text Generation\n",
    "if 'dance_encoder' in locals() and 'text_encoder' in locals() and 'sequences' in locals() and 'test_text_embeddings' in locals():\n",
    "    \"\"\"\n",
    "    ## Dance to Text Generation\n",
    "    \n",
    "    Now let's demonstrate the reverse: given a dance sequence, generate an appropriate text description.\n",
    "    We can:\n",
    "    1. Encode the dance sequence with our dance encoder\n",
    "    2. Find the closest text embeddings in our shared space\n",
    "    3. Return the corresponding text descriptions\n",
    "    \n",
    "    For more natural-sounding descriptions, we can also create a composite description from multiple\n",
    "    top matches.\n",
    "    \"\"\"\n",
    "    # Choose random test sequences to describe\n",
    "    n_samples = 3\n",
    "    if 'test_dance_embeddings' in locals() and test_dance_embeddings is not None:\n",
    "        sample_indices = random.sample(range(len(test_dance_embeddings)), k=min(n_samples, len(test_dance_embeddings)))\n",
    "    else:\n",
    "        sample_indices = random.sample(range(len(sequences)), k=min(n_samples, len(sequences)))\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        print(f\"\\nDance Sequence {idx}:\")\n",
    "        \n",
    "        # Get the sequence\n",
    "        dance_seq = sequences[idx]\n",
    "        \n",
    "        # Retrieve matching text\n",
    "        results = retrieve_text_by_dance(\n",
    "            dance_seq,\n",
    "            dance_encoder,\n",
    "            test_text_embeddings,\n",
    "            all_labels,\n",
    "            top_k=3,  # Get top 3 matches\n",
    "            device=device\n",
    "        )\n",
    "        # Display results\n",
    "        if results:\n",
    "            print(f\"Top matches:\")\n",
    "            for i, (match_idx, score, text) in enumerate(results):\n",
    "                print(f\"  {i+1}. \\\"{text}\\\" (score: {score:.4f})\")\n",
    "            \n",
    "            # Generate composite description\n",
    "            composite = generate_composite_description(\n",
    "                dance_seq,\n",
    "                dance_encoder,\n",
    "                test_text_embeddings,\n",
    "                all_labels,\n",
    "                num_components=3,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nComposite description: \\\"{composite}\\\"\")\n",
    "            \n",
    "            # Create animation (but don't display in this code snippet)\n",
    "            print(\"Animation would be displayed here in notebook\")\n",
    "            # ani = animator.animate_sequence(dance_seq)\n",
    "            # display(HTML(ani.to_jshtml()))\n",
    "        else:\n",
    "            print(\"No matching text found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating between sequences:\n",
      "Sequence 708 - Label: \"sharp swing with feet moving high\"\n",
      "Sequence 1628 - Label: \"heavy bend with back moving low\"\n",
      "\n",
      "Created 5 interpolated sequences\n",
      "Animations would be displayed here in notebook\n",
      "\n",
      "Text descriptions for interpolated sequences:\n",
      "  100% seq1, 0% seq2 → \"fluid stretch and turn\"\n",
      "  75% seq1, 25% seq2 → \"fluid stretch and turn\"\n",
      "  50% seq1, 50% seq2 → \"fluid stretch and turn\"\n",
      "  25% seq1, 75% seq2 → \"fluid stretch and turn\"\n",
      "  0% seq1, 100% seq2 → \"fluid stretch and turn\"\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Interpolation in Embedding Space\n",
    "if 'dance_encoder' in locals() and 'text_encoder' in locals() and 'sequences' in locals():\n",
    "    \"\"\"\n",
    "    ## Creative Applications: Embedding Space Interpolation\n",
    "    \n",
    "    One interesting creative application is interpolating between different dance movements\n",
    "    in the embedding space. This allows us to:\n",
    "    1. Blend characteristics of different dance sequences\n",
    "    2. Create smooth transitions between movements\n",
    "    3. Explore the \"space\" between different choreographic elements\n",
    "    \n",
    "    We can do this by:\n",
    "    1. Selecting two or more source dance sequences\n",
    "    2. Finding their locations in the embedding space\n",
    "    3. Interpolating between those points\n",
    "    4. Finding the closest actual dance sequence to the interpolated point\n",
    "    \n",
    "    Alternatively, we can directly interpolate in the sequence space by blending joint positions.\n",
    "    \"\"\"\n",
    "    # For demonstration, we'll use direct sequence interpolation\n",
    "    from src.generation.dance_from_text import interpolate_dance_sequences\n",
    "    \n",
    "    # Select two sequences to interpolate between (random selection)\n",
    "    if len(sequences) >= 2:\n",
    "        idx1, idx2 = random.sample(range(len(sequences)), k=2)\n",
    "        seq1 = sequences[idx1]\n",
    "        seq2 = sequences[idx2]\n",
    "        \n",
    "        print(f\"Interpolating between sequences:\")\n",
    "        print(f\"Sequence {idx1} - Label: \\\"{all_labels[idx1]}\\\"\")\n",
    "        print(f\"Sequence {idx2} - Label: \\\"{all_labels[idx2]}\\\"\")\n",
    "        \n",
    "        # Create interpolated sequences\n",
    "        weights = [\n",
    "            [1.0, 0.0],    # 100% seq1, 0% seq2\n",
    "            [0.75, 0.25],  # 75% seq1, 25% seq2\n",
    "            [0.5, 0.5],    # 50% seq1, 50% seq2\n",
    "            [0.25, 0.75],  # 25% seq1, 75% seq2\n",
    "            [0.0, 1.0]     # 0% seq1, 100% seq2\n",
    "        ]\n",
    "        interpolated_seqs = []\n",
    "        for w in weights:\n",
    "            interp = interpolate_dance_sequences([seq1, seq2], w)\n",
    "            interpolated_seqs.append(interp)\n",
    "        \n",
    "        print(f\"\\nCreated {len(interpolated_seqs)} interpolated sequences\")\n",
    "        print(\"Animations would be displayed here in notebook\")\n",
    "        \n",
    "        # Generate text descriptions for interpolated sequences\n",
    "        if 'test_text_embeddings' in locals() and test_text_embeddings is not None:\n",
    "            print(\"\\nText descriptions for interpolated sequences:\")\n",
    "            for i, seq in enumerate(interpolated_seqs):\n",
    "                w = weights[i]\n",
    "                desc = generate_composite_description(\n",
    "                    seq,\n",
    "                    dance_encoder,\n",
    "                    test_text_embeddings,\n",
    "                    all_labels,\n",
    "                    num_components=2,\n",
    "                    device=device\n",
    "                )\n",
    "                print(f\"  {w[0]*100:.0f}% seq1, {w[1]*100:.0f}% seq2 → \\\"{desc}\\\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7: Summary and Future Directions\n",
    "\"\"\"\n",
    "## Summary and Conclusion\n",
    "\n",
    "In this project, we've successfully:\n",
    "\n",
    "1. **Visualized dance motion capture data** with interactive 3D animations that show the dancer's movement through time.\n",
    "\n",
    "2. **Developed a labeling strategy** that uses semi-supervised learning to generate descriptive text for dance sequences.\n",
    "\n",
    "3. **Trained a contrastive learning model** that embeds both dance movements and text descriptions in a shared space.\n",
    "\n",
    "4. **Demonstrated bidirectional generation** by retrieving dance sequences from text and generating text descriptions for dance movements.\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "There are several exciting directions for further development:\n",
    "\n",
    "1. **Improved Dance Representation**:\n",
    "   - Use more sophisticated temporal models (e.g., LSTMs, Transformers) to better capture the sequential nature of dance\n",
    "   - Incorporate physical constraints and biomechanical principles into the encoding\n",
    "\n",
    "2. **Enhanced Text Processing**:\n",
    "   - Use pre-trained language models (e.g., BERT, RoBERTa) for more nuanced text understanding\n",
    "   - Develop a more specialized vocabulary for dance movements\n",
    "\n",
    "3. **Generative Models**:\n",
    "   - Move beyond retrieval to truly generative models that can synthesize novel dance sequences\n",
    "   - Incorporate music as a third modality for multimodal generation\n",
    "\n",
    "4. **Interactive Applications**:\n",
    "   - Develop creative tools that allow choreographers to explore the dance-text space\n",
    "   - Create interactive installations where viewers can generate dance through language\n",
    "\n",
    "This project demonstrates the potential of AI to bridge movement and language, opening new possibilities for both choreographic exploration and dance documentation.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
